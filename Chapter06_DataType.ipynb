{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6. 다양한 데이터 타입 다루기\n",
    "## 6.1 API는 어디서 찾을까?\n",
    "+ DataFrame 메서드\n",
    "    + DataFrameStatFunctions : 다양한 통계적 함수를 제공\n",
    "    + DataFrameNaFunctions : null 데이터를 다루는데 필요한 함수를 제공\n",
    "+ Column 메서드\n",
    "    + alias, contains과 같은 컬럼과 관련된 여러가지 메서드를 제공\n",
    "    + org.apache.spark.sql.function 패키지는 데이터 타입과 관련된 다양한 함수를 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" DataFrame 생성 \"\"\"\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"../BookSamples/data/retail-data/by-day/2010-12-01.csv\")\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 스파크 데이터 타입으로 변환하기\n",
    "+ lit 함수 : 다른 언어의 데이터 타입을 스파크 데이터 타입에 맞게 변환\n",
    "+ SQL에서는 스파크 데이터 타입으로 변환할 필요가 없으므로 값을 직접 입력해 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[5: int, five: string, 5.0: double]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 스파크 데이터 타입으로 변환 \"\"\"\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df.select(lit(5), lit(\"five\"), lit(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 불리언 데이터 타입 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+\n",
      "|InvoiceNO|Description                  |\n",
      "+---------+-----------------------------+\n",
      "|536366   |HAND WARMER UNION JACK       |\n",
      "|536366   |HAND WARMER RED POLKA DOT    |\n",
      "|536367   |ASSORTED COLOUR BIRD ORNAMENT|\n",
      "|536367   |POPPY'S PLAYHOUSE BEDROOM    |\n",
      "|536367   |POPPY'S PLAYHOUSE KITCHEN    |\n",
      "+---------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n",
      "|536365   |71053    |WHITE METAL LANTERN                |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |2010-12-01 08:26:00|2.75     |17850.0   |United Kingdom|\n",
      "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 조건절 입력 \"\"\"\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 컬럼 함수 사용\n",
    "df.where(col(\"InvoiceNO\") != 536365) \\\n",
    "    .select(\"InvoiceNO\", \"Description\") \\\n",
    "    .show(5, False)\n",
    "\n",
    "# 가장 명확한 방법은 문자열 표현식에 조건절을 명시하는 것\n",
    "df.where(\"InvoiceNO = 536365\") \\\n",
    "    .show(5, False) # truncate = False\n",
    "\n",
    "df.where(\"InvoiceNO <> 536365\") \\\n",
    "    .show(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" an, or 사용한 불리언 표현식 \"\"\"\n",
    "\n",
    "from pyspark.sql.functions import instr # contains\n",
    "\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(df.Description, \"POSTAGE\") >= 1 # Locate the position of the first occurrence of substr column in the given string.\n",
    "                                                      # Returns null if either of the arguments are null.\n",
    "df.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descripFilter).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|added|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|    8|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|    8|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" instr 함수 \"\"\"\n",
    "\n",
    "df.withColumn(\"added\", instr(df.Description, \"POSTAGE\")).where(\"added > 1\").show() # 8번째 글자에 'POSTAGE'가 시작됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|unitPrice|isExpensive|\n",
      "+---------+-----------+\n",
      "|   569.77|       true|\n",
      "|   607.49|       true|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 불리언 컬럼 \"\"\"\n",
    "from pyspark.sql.functions import instr\n",
    "\n",
    "DOTCodeFilter = col(\"StockCode\") == \"DOT\" # column type\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(col(\"Description\"), \"POSTAGE\") > 1 # 'POSTAGE' 글자를 포함하고 있는 경우\n",
    "df.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter)) \\\n",
    "    .where(\"isExpensive\") \\\n",
    "    .select(\"unitPrice\", \"isExpensive\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'(StockCode = DOT)'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col(\"StockCode\") == \"DOT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [Description#18, UnitPrice#21]\n",
      "+- *(1) Filter (isnotnull(UnitPrice#21) AND (UnitPrice#21 > 250.0))\n",
      "   +- FileScan csv [Description#18,UnitPrice#21] Batched: false, DataFilters: [isnotnull(UnitPrice#21), (UnitPrice#21 > 250.0)], Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/work/BookSamples/data/retail-data/by-day/2010-12-01.csv], PartitionFilters: [], PushedFilters: [IsNotNull(UnitPrice), GreaterThan(UnitPrice,250.0)], ReadSchema: struct<Description:string,UnitPrice:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" SQL문 실행 \"\"\"\n",
    "from pyspark.sql.functions import expr, col # 파이썬은 not이 존재하지 않는다. \n",
    "\n",
    "df.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\")) \\\n",
    "    .where(\"isExpensive\") \\\n",
    "    .select(\"Description\", \"UnitPrice\").explain()\n",
    "# SQL을 사용해도 성능은 차이나지 않는다. -> spark 고유의 데이터 타입 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 수치형 데이터 타입 다루기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 아래 예제의 두 컬럼 모두 수치형이므로 곱셈 연산이 가능함. 필요한 경우 덧셈이나 뺄셈도 가능\n",
    "+ SQL을 사용해 동일하게 처리할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerID|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "|   17850.0|             489.0|\n",
      "|   17850.0|          418.7156|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 지수만큼 제곱하는 pow 함수 \"\"\"\n",
    "from pyspark.sql.functions import expr, pow\n",
    "\n",
    "fabricateQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n",
    "df.select(expr(\"CustomerID\"), fabricateQuantity.alias(\"realQuantity\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" SQL문으로 위와 동일한 결과 실행 \"\"\"\n",
    "df.selectExpr(\n",
    "    \"CustomerId\",\n",
    "    \"(POWER(Quantity * UnitPrice, 2.0) + 5) as realQuantity\"\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ round: 소수점 값이 정확히 중간값 이상이라면 반올림\n",
    "+ bround: 반내림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "+-------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------+--------------+\n",
      "|round(2.5, 1)|bround(2.5, 1)|\n",
      "+-------------+--------------+\n",
      "|          2.5|           2.5|\n",
      "|          2.5|           2.5|\n",
      "+-------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------+---------------+\n",
      "|round(2.55, 1)|bround(2.55, 1)|\n",
      "+--------------+---------------+\n",
      "|           2.6|            2.6|\n",
      "|           2.6|            2.6|\n",
      "+--------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 반올림하는 round 함수 \"\"\"\n",
    "from pyspark.sql.functions import lit, round, bround # round(반올림), bround(반내림)\n",
    "\n",
    "df.select(round(lit(\"2.5\"), 0), bround(lit(\"2.5\"), 0)).show(2) \n",
    "df.select(round(lit(\"2.5\"), 1), bround(lit(\"2.5\"), 1)).show(2) # 1차원\n",
    "df.select(round(lit(\"2.55\"), 1), bround(lit(\"2.55\"), 1)).show(2) #??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ class pyspark.sql.DataFrameStatFunctions : Functionality for statistic functions with DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.04112314436835551"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 피어슨 상관계수를 계산 \"\"\"\n",
    "from pyspark.sql.functions import corr\n",
    "\n",
    "df.stat.corr(\"Quantity\", \"UnitPrice\")\n",
    "df.corr(\"Quantity\", \"UnitPrice\") # dataframe에서 바로 접근도 가능함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 요약 통계는 describe메서드를 사용해 얻을 수 있음. \n",
    "+ describe메서드는 관련 컬럼에 대한 집계(count), 평균(mean), 표준편차(stddev), 최솟값(min), 최댓값(max)를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|        InvoiceDate|         UnitPrice|        CustomerID|       Country|\n",
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|  count|             3108|              3108|                3098|              3108|               3108|              3108|              1968|          3108|\n",
      "|   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128|               null| 4.151946589446603|15661.388719512195|          null|\n",
      "| stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|               null|15.638659854603892|1854.4496996893627|          null|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|2010-12-01 08:26:00|               0.0|           12431.0|     Australia|\n",
      "|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|2010-12-01 17:35:00|            607.49|           18229.0|United Kingdom|\n",
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "\n",
      "+-------+-----------------+\n",
      "|summary|        InvoiceNo|\n",
      "+-------+-----------------+\n",
      "|  count|             3108|\n",
      "|   mean| 536516.684944841|\n",
      "| stddev|72.89447869788873|\n",
      "|    min|           536365|\n",
      "|    max|          C536548|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 요약 통계를 계산 \"\"\"\n",
    "df.describe().show()\n",
    "df.describe(\"InvoiceNo\").show() # 컬럼을 입력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 정확한 수치가 필요하다면 함수를 임포트하고 해당 컬럼에 적용하는 방식으로 직접 집계를 수행할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|stddev_pop(UnitPrice)|\n",
      "+---------------------+\n",
      "|   15.636143780280698|\n",
      "+---------------------+\n",
      "\n",
      "+---------------------+\n",
      "|stddev_pop(UnitPrice)|\n",
      "+---------------------+\n",
      "|   15.636143780280698|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, mean, stddev_pop, min, max\n",
    "\n",
    "df.select(stddev_pop(col(\"UnitPrice\"))).show()\n",
    "df.select(stddev_pop(\"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ StaFunctions 패키지는 다양한 통계 함수를 제공\n",
    "+ stat 속성을 사용해 접근할 수 있으며 다양한 통곗값을 계산할 때 사용하는 DataFrame 메서드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ approxQuantile:\n",
    "    + The result of this algorithm has the following deterministic bound: If the DataFrame has N elements and if we request the quantile at probability p up to error err, then the algorithm will return a sample x from the DataFrame so that the exact rank of x is close to (p * N). \n",
    "    + More precisely, `floor((p - err) * N) <= rank(x) <= ceil((p + err) * N)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.51]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 백분위수를 구하는 approxQuantile \"\"\"\n",
    "olName = \"UnitPrice\"\n",
    "quantileProbs = [0.5]\n",
    "relError = 0.05\n",
    "\n",
    "df.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError)\n",
    "# :relError: The relative target precision to achieve\n",
    "#   (>= 0). If set to zero, the exact quantiles are computed, which\n",
    "#   could be very expensive. Note that values greater than 1 are\n",
    "#   accepted but give the same result as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|    21773|DECORATIVE ROSE B...|       1|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    21774|DECORATIVE CATS B...|       2|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    21791|VINTAGE HEADS AND...|       2|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    21809|CHRISTMAS HANGING...|       1|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    21810|CHRISTMAS HANGING...|       3|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    21811|CHRISTMAS HANGING...|       1|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    21889|WOODEN BOX OF DOM...|       2|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    21892|TRADITIONAL WOODE...|       3|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    21894|POTTING SHED SEED...|       1|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    21914|BLUE HARMONICA IN...|       3|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    21915|RED  HARMONICA IN...|       2|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    21949|SET OF 6 STRAWBER...|       1|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    21990|MODERN FLORAL STA...|       2|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    21991|BOHEMIAN COLLAGE ...|       1|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    21992|VINTAGE PAISLEY S...|       6|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    21993|FLORAL FOLK STATI...|       4|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    22095|LADS ONLY TISSUE BOX|       2|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    22100|SKULLS SQUARE TIS...|       1|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    22178|VICTORIAN GLASS H...|      13|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "|   536544|    22190|      LOCAL CAFE MUG|       3|2010-12-01 14:32:00|     2.51|      null|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"UnitPrice = 2.51\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ crosstab : Computes a pair-wise frequency table of the given columns. Also known as a contingency table. \n",
    "    + The first column of each row will be the distinct values of col1 and the column names will be the distinct values of col2. \n",
    "    + The name of the first column will be `$col1_$col2`. \n",
    "    + Pairs that have no occurrences will have zero as their counts. \n",
    "    + DataFrame.crosstab() and DataFrameStatFunctions.crosstab() are aliases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|StockCode|\n",
      "+---------+\n",
      "|    22728|\n",
      "|    21889|\n",
      "|   90210B|\n",
      "|    21259|\n",
      "|    21894|\n",
      "|    21452|\n",
      "|    22121|\n",
      "|    90022|\n",
      "|    21249|\n",
      "|    21711|\n",
      "|    22130|\n",
      "|    22314|\n",
      "|    21671|\n",
      "|    22629|\n",
      "|    82486|\n",
      "|    22438|\n",
      "|    22529|\n",
      "|    10133|\n",
      "|    21967|\n",
      "|    85064|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+\n",
      "|Quantity|\n",
      "+--------+\n",
      "|      34|\n",
      "|      -1|\n",
      "|      28|\n",
      "|      27|\n",
      "|     384|\n",
      "|     -10|\n",
      "|     192|\n",
      "|      12|\n",
      "|     128|\n",
      "|      22|\n",
      "|      47|\n",
      "|       1|\n",
      "|      13|\n",
      "|       6|\n",
      "|      16|\n",
      "|       3|\n",
      "|      20|\n",
      "|      40|\n",
      "|      -7|\n",
      "|     432|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|StockCode_Quantity| -1|-10|-12| -2|-24| -3| -4| -5| -6| -7|  1| 10|100| 11| 12|120|128| 13| 14|144| 15| 16| 17| 18| 19|192|  2| 20|200| 21|216| 22| 23| 24| 25|252| 27| 28|288|  3| 30| 32| 33| 34| 36|384|  4| 40|432| 47| 48|480|  5| 50| 56|  6| 60|600| 64|  7| 70| 72|  8| 80|  9| 96|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|             22578|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21327|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22064|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21080|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|\n",
      "|             22219|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21908|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22818|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|           15056BL|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             72817|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22545|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22988|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|\n",
      "|             22274|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             20750|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|            82616C|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21703|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22899|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|\n",
      "|             22379|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22422|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22769|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22585|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 교차표를 생성하는 crosstab \"\"\"\n",
    "df.select(\"StockCode\").distinct().show() # rows name\n",
    "df.select(\"Quantity\").distinct().show()  # columns name\n",
    "df.stat.crosstab(\"StockCode\", \"Quantity\").show() # pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ monotonically_increasing_id:\n",
    "    + 모든 로우에 고유 ID값을 추가함. 모든 로우에 0부터 시작하는 고윳값을 생성\n",
    "    + The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive. \n",
    "    + The current implementation puts the partition ID in the upper 31 bits, and the record number within each partition in the lower 33 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|monotonically_increasing_id()|\n",
      "+-----------------------------+\n",
      "|                            0|\n",
      "|                            1|\n",
      "|                            2|\n",
      "|                            3|\n",
      "|                            4|\n",
      "|                            5|\n",
      "|                            6|\n",
      "|                            7|\n",
      "|                            8|\n",
      "|                            9|\n",
      "+-----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 고유 아이디를 생성하는 monotonically_increasing_id \"\"\"\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df.select(monotonically_increasing_id()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------+\n",
      "|        id|\n",
      "+----------+\n",
      "|8589936145|\n",
      "|8589936144|\n",
      "|8589936143|\n",
      "|8589936142|\n",
      "|8589936141|\n",
      "|8589936140|\n",
      "|8589936139|\n",
      "|8589936138|\n",
      "|8589936137|\n",
      "|8589936136|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.repartition(2)\n",
    "df2.select(monotonically_increasing_id().alias('id')) \\\n",
    "    .show(10)\n",
    "df2.select(monotonically_increasing_id().alias('id')) \\\n",
    "    .orderBy(col('id').desc()) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 문자열 데이터 타입 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|initcap(Description)              |\n",
      "+----------------------------------+\n",
      "|White Hanging Heart T-light Holder|\n",
      "|White Metal Lantern               |\n",
      "+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 공백으로 나뉘는 모든 단어의 첫 글자를 대문자로 변경, initcap \"\"\" \n",
    "from pyspark.sql.functions import initcap\n",
    "\n",
    "df.select(initcap(col(\"Description\"))).show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|         Description|  lower(Description)|  upper(Description)|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|WHITE HANGING HEA...|white hanging hea...|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN| white metal lantern| WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 소문자/대문자 변경, lower/upper \"\"\"\n",
    "from pyspark.sql.functions import lower, upper\n",
    "\n",
    "df.select(col(\"Description\"), lower(col(\"Description\")), upper(col(\"Description\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+---+----------+\n",
      "|   ltrim|   rtrim| trim| lp|        rp|\n",
      "+--------+--------+-----+---+----------+\n",
      "|HELLO   |   HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO   |   HELLO|HELLO|HEL|HELLO     |\n",
      "+--------+--------+-----+---+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 문자열 주변의 공백을 제거, lpad/ltrim/rpad/rtrim/trim \"\"\"\n",
    "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
    "\n",
    "df.select(\n",
    "    ltrim(lit(\"   HELLO   \")).alias(\"ltrim\"),\n",
    "    rtrim(lit(\"   HELLO   \")).alias(\"rtrim\"),\n",
    "    trim(lit(\"   HELLO   \")).alias(\"trim\"),\n",
    "    lpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\n",
    "    rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.1 정규 표현식\n",
    "+ 존재 여부를 확인하거나 일치하는 모든 문자열을 치환\n",
    "+ 정규 표현식을 위해 `regexp_extract` 함수와 `regexp_replace` 함수를 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         color_clean|         Description|\n",
      "+--------------------+--------------------+\n",
      "|COLOR HANGING HEA...|WHITE HANGING HEA...|\n",
      "| COLOR METAL LANTERN| WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 단어 치환, regexp_extract \"\"\"\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "regex_string = \"BLACK|WHITE|RED|GRENN|BLUE\"\n",
    "df.select(\n",
    "    regexp_replace(col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\"),\n",
    "    col(\"Description\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------------------+\n",
      "|translate(Description, LEET, 12)|         Description|\n",
      "+--------------------------------+--------------------+\n",
      "|            WHI2 HANGING H2AR...|WHITE HANGING HEA...|\n",
      "|                WHI2 M2A1 1AN2RN| WHITE METAL LANTERN|\n",
      "+--------------------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 문자 단위 치환, translate \"\"\"\n",
    "from pyspark.sql.functions import translate\n",
    "\n",
    "df.select(translate(col(\"Description\"), \"LEET\", \"12\"), # 매칭되지 않아도 실행되니 주의 (L=1, E=2, T='' 으로 치환됨)\n",
    "          col(\"Description\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ regexp_extract(str, pattern, idx):\n",
    "    + Extract a specific group matched by a Java regex, from the specified string column. If the regex did not match, or the specified group did not match, an empty string is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|color_clean|         Description|\n",
      "+-----------+--------------------+\n",
      "|      WHITE|WHITE HANGING HEA...|\n",
      "|      WHITE| WHITE METAL LANTERN|\n",
      "+-----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 단어 추출, regexp_extract \"\"\"\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "# 처음 나타난 색상 이름을 추출\n",
    "extract_str = \"(BLACK|WHITE|RED|GRENN|BLUE)\"\n",
    "df.select(\n",
    "    regexp_extract(col(\"Description\"), extract_str, 1).alias(\"color_clean\"),\n",
    "    col(\"Description\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 단어 존재유무, contain \"\"\" # 파이썬과 SQL은 instr 함수를 사용\n",
    "from pyspark.sql.functions import instr\n",
    "\n",
    "containBlack = instr(col(\"Description\"), \"BLACK\") >= 1\n",
    "containWhite = instr(col(\"Description\"), \"WHITE\") >= 1\n",
    "df.withColumn(\"hasSimpleColor\", containBlack | containWhite) \\\n",
    "    .where(\"hasSimpleColor\") \\\n",
    "    .select(\"Description\").show(3, False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 동적으로 인수의 개수가 변화는 상황을 스파크는 어떻게 처리하나?\n",
    "    + 값 목록을 인수로 변환해 함수에 전달할 때는 varargs라 불리는 스칼라 고유 기능을 활용 -> 임의 길이의 배열을 효율적으로 다룰 수 있음\n",
    "    + 파이썬은 인수의 개수가 동적으로 변화는 상황을 아주 쉽게 해결할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 문자열의 위치(위치는 1부터 시작)를 정수로 반환하는 locate 함수를 사용, 그런다음 위치 정보를 불리언 타입으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+--------+-------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|is_black|is_white|is_red|is_green|is_blue|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+--------+--------+------+--------+-------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   false|    true| false|   false|  false|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   false|    true| false|   false|  false|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   false|   false| false|   false|  false|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "+--------+--------+------+--------+-------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 인수의 개수가 동적으로 변하는 상황과 문자열의 위치를 찾는 locate 함수 \"\"\"\n",
    "from pyspark.sql.functions import expr, locate \n",
    "\n",
    "simple_colors = [\"black\", \"white\", \"red\", \"green\", \"blue\"]\n",
    "def color_locator(column, color_string):\n",
    "    return locate(color_string.upper(), column).cast(\"boolean\").alias(\"is_\" + color_string) # color_strings 단어가 시작되는 문자기준(단어기준 X) 위치\n",
    "selected_cols = [color_locator(df.Description, c) for c in simple_colors] # locate 함수를 하나씩 list에 저장\n",
    "selected_cols.append(expr(\"*\")) # column 타입이여야 함 \n",
    "\n",
    "df.select(*selected_cols).show(3)\n",
    "\n",
    "df.select(*selected_cols).where(expr(\"is_white OR is_red\")) \\\n",
    "    .select(col(\"Description\")).show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column<b'CAST(locate(BLACK, Description, 1) AS BOOLEAN) AS `is_black`'>,\n",
       " Column<b'CAST(locate(WHITE, Description, 1) AS BOOLEAN) AS `is_white`'>,\n",
       " Column<b'CAST(locate(RED, Description, 1) AS BOOLEAN) AS `is_red`'>,\n",
       " Column<b'CAST(locate(GREEN, Description, 1) AS BOOLEAN) AS `is_green`'>,\n",
       " Column<b'CAST(locate(BLUE, Description, 1) AS BOOLEAN) AS `is_blue`'>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_cols = [color_locator(df.Description, c) for c in simple_colors] # locate 함수를 하나씩 list에 저장\n",
    "selected_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 날짜와 타임스팸프 데이터 타입 다루기\n",
    "+ 스파크는 2가지 시간 정보만 다룸\n",
    "    + 날짜 정보만 가지는 date\n",
    "    + 날짜와 시간 정보를 모두 가지는 timestamp\n",
    "+ 시간대 설정이 필요하다면 스파크 SQL 설정의 spark.conf.sessionLocalTimeZone 속성으로 가능\n",
    "    + [자바 TimeZone 포맷을 따라야 함](https://docs.oracle.com/javase/7/docs/api/java/util/TimeZone.html)\n",
    "+ TimestampType 클래스는 초 단위 정밀도만 지원   \n",
    "    + 초 단위 이상 정밀도 요구 시 long 데이터 타입으로 데이터를 변환해 처리하는 우회 정책이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n",
      "+---+----------+--------------------------+\n",
      "|id |today     |now                       |\n",
      "+---+----------+--------------------------+\n",
      "|0  |2020-11-14|2020-11-14 23:55:07.602953|\n",
      "|1  |2020-11-14|2020-11-14 23:55:07.602953|\n",
      "|2  |2020-11-14|2020-11-14 23:55:07.602953|\n",
      "+---+----------+--------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 오늘 날짜와 현재의 타임스탬프값 구하기 \"\"\"\n",
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "dateDF = spark.range(10) \\\n",
    "    .withColumn(\"today\", current_date())\\\n",
    "    .withColumn(\"now\", current_timestamp())\n",
    "dateDF.createOrReplaceTempView(\"dataTable\")\n",
    "dateDF.printSchema()\n",
    "\n",
    "dateDF.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2020-11-09|        2020-11-19|\n",
      "+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 오늘을 기준으로 날짜를 더하거나 빼기 \"\"\"\n",
    "from pyspark.sql.functions import date_sub, date_add\n",
    "\n",
    "dateDF.select(\n",
    "    date_sub(col(\"today\"), 5),\n",
    "    date_add(col(\"today\"), 5)\n",
    ").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------------------+\n",
      "|months_between(start, end, true)|\n",
      "+--------------------------------+\n",
      "|                    -16.67741935|\n",
      "+--------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 두 날짜 사이의 일/개월 수를 파악 \"\"\"\n",
    "from pyspark.sql.functions import datediff, months_between, to_date\n",
    "\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n",
    "    .select(datediff(col(\"week_ago\"), col(\"today\"))).show(1) # 현재 날짜에서 7일 제외 후 datediff 결과 확인\n",
    "dateDF.select(\n",
    "    to_date(lit(\"2016-01-01\")).alias(\"start\"), \n",
    "    to_date(lit(\"2017-05-22\")).alias(\"end\"))\\\n",
    "    .select(months_between(col(\"start\"), col(\"end\"))).show(1) # 개월 수 차이 파악"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ to_date: 문자열을 날짜로 변경할 수 있으며, 필요에 따라 날짜 포맷도 함께 지정하 수 있음.\n",
    "    + 함수의 날짜 포맷은 반드시 자바의 SimpleDateFormat클래스가 지원하는 포맷을 사용해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- to_date(`date`): date (nullable = true)\n",
      "\n",
      "+---------------+\n",
      "|to_date(`date`)|\n",
      "+---------------+\n",
      "|     2017-01-01|\n",
      "+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 문자열을 날짜로 변환 \"\"\" # 자바의 simpleDateFormat 클래스가 지원하는 포맷 사용 필요\n",
    "from pyspark.sql.functions import to_date, lit\n",
    "\n",
    "df_date = spark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\\\n",
    "    .select(to_date(col(\"date\")))\n",
    "df_date.printSchema()\n",
    "df_date.show(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 날짜를 파싱할 수 없다면 에러 대신 null값을 반환함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|to_date('2016-20-12')|to_date('2017-12-11')|\n",
      "+---------------------+---------------------+\n",
      "|                 null|           2017-12-11|\n",
      "+---------------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 파싱오류로 날짜가 null로 반환되는 사례 \"\"\"\n",
    "dateDF.select(to_date(lit(\"2016-20-12\")), to_date(lit(\"2017-12-11\"))).show(1) # 월과 일의 순서가 바뀜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 날짜 형식을 지키지 않은 데이터가 들어온다면 디버깅하기 매우 어려움\n",
    "    + 자바의 SimpleDateFormat 표준에 맞춰 날짜 포맷 지정\n",
    "    + to_date 함수는 필요에 따라 날짜 포맷을 지정할 수 있지만, to_timestamp함수는 반드시 날짜 포맷을 지정해야 함\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SimpleDateFormat 표준을 활용하여 날짜 포멧을 지정 \"\"\"\n",
    "from pyspark.sql.functions import to_date\n",
    "dateFormat = \"yyyy-dd-MM\" # 소문자 mm 주의\n",
    "cleanDateDF = spark.range(1).select( # 1개 Row를 생성\n",
    "    to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\n",
    "    to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n",
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ SimpleDateFormat : https://bvc12.tistory.com/168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * from dateTable2\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|to_timestamp(`date`, 'yyyy-dd-MM')|\n",
      "+----------------------------------+\n",
      "|               2017-11-12 00:00:00|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 항상 날짜 포맷을 지정해야 하는 to_timestamp 함수 \"\"\"\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "cleanDateDF.select(to_timestamp(col(\"date\"), dateFormat)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 날짜 비교 \"\"\"\n",
    "cleanDateDF.filter(col(\"date2\") > lit(\"2017-12-12\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 null 값 다루기\n",
    "+ null 값을 사용하는 것 보다 명시적으로 사용하는 것이 항상 좋음\n",
    "+ null 값을 허용하지 않는 컬럼을 선언해도 강제성은 없음\n",
    "+ nullable 속성은 스파크 SQL 옵티마이저가 해당 컬럼을 제어하는 동작을 단순하게 돕는 역할\n",
    "+ null 값을 다루는 방법은 두 가지 \n",
    "    + 명시적으로 null을 제거\n",
    "    + 전역 또느 컬럼 단위로 null 값을 특정 값으로 채움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7.1 coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|coalesce(Description, CustomerId)|\n",
      "+---------------------------------+\n",
      "|             WHITE HANGING HEA...|\n",
      "|              WHITE METAL LANTERN|\n",
      "|             CREAM CUPID HEART...|\n",
      "|             KNITTED UNION FLA...|\n",
      "|             RED WOOLLY HOTTIE...|\n",
      "|             SET 7 BABUSHKA NE...|\n",
      "|             GLASS STAR FROSTE...|\n",
      "|             HAND WARMER UNION...|\n",
      "|             HAND WARMER RED P...|\n",
      "|             ASSORTED COLOUR B...|\n",
      "|             POPPY'S PLAYHOUSE...|\n",
      "|             POPPY'S PLAYHOUSE...|\n",
      "|             FELTCRAFT PRINCES...|\n",
      "|             IVORY KNITTED MUG...|\n",
      "|             BOX OF 6 ASSORTED...|\n",
      "|             BOX OF VINTAGE JI...|\n",
      "|             BOX OF VINTAGE AL...|\n",
      "|             HOME BUILDING BLO...|\n",
      "|             LOVE BUILDING BLO...|\n",
      "|             RECIPE BOX WITH M...|\n",
      "+---------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce # 인수로 지정한 여러 컬럼 중 null이 아닌 첫번 째 값 반환\n",
    "\n",
    "df.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7.2 ifnull, nullIf, nvl, nvl2\n",
    "+ SQL 함수이며 DataFrame의 select 표현식으로 사용 가능\n",
    "    + ifnull(null, 'return_value') # 두 번째 값을, 아니라면 첫 번째 값을 반환 \n",
    "    + nullif('value', 'value')     # 두 값이 같으면 null\n",
    "    + nvl(null, 'return_value')    # 두 번째 값을, 아니라면 첫 번째 값을 반환\n",
    "    + nvl2('not_null', 'return_value', 'else_value') # 두 번째 값을, 아니라면 세번째 값을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------+-------------------------+----------------------------------------------+\n",
      "|ifnull(NULL, 'return_value')|nullif('value', 'value')|nvl(NULL, 'return_value')|nvl2('not null', 'return_value', 'else_value')|\n",
      "+----------------------------+------------------------+-------------------------+----------------------------------------------+\n",
      "|                return_value|                    null|             return_value|                                  return_value|\n",
      "+----------------------------+------------------------+-------------------------+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    ifnull(null, 'return_value'),\n",
    "    nullif('value', 'value'),\n",
    "    nvl(null, 'return_value'),\n",
    "    nvl2('not null', 'return_value', 'else_value')\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7.3 drop\n",
    "+ null 값을 가진 로우를 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop()\n",
    "df.na.drop(\"any\").show(1) # 로우 컬럼값 중 하나라도 null이면 제거\n",
    "df.na.drop(\"all\").show(1) # 로우 컬럼값 모두 null이면 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(\"all\", subset=(\"StockCode\", \"InvoiceNo\")).show(1) # 배열 형태의 컬럼을 인수로 전달 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7.4 fill\n",
    "+ fill: null을 특정한 값으로 채움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------+\n",
      "|string_null|string2_null|number_null|\n",
      "+-----------+------------+-----------+\n",
      "|      Hello|        null|        5.0|\n",
      "|       null|       World|       null|\n",
      "+-----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" null을 포함한 DataFrame 행성 \"\"\"\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "    StructField(\"string_null\", StringType(), True),\n",
    "    StructField(\"string2_null\", StringType(), True),\n",
    "    StructField(\"number_null\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "myRows = []\n",
    "myRows.append(Row(\"Hello\", None, float(5))) # string 컬럼에 null 포함\n",
    "myRows.append(Row(None, \"World\", None))     # number 컬럼에 null 포함\n",
    "\n",
    "myDf = spark.createDataFrame(myRows, myManualSchema)\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ fill 함수는 DataType이 동일한 컬럼의 null만 치완\n",
    "+ 숫자형 또한 치환할 값의 DataType이 동일해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+\n",
      "|         string_null|        string2_null|number_null|\n",
      "+--------------------+--------------------+-----------+\n",
      "|               Hello|All null valus be...|        5.0|\n",
      "|All null valus be...|               World|       null|\n",
      "+--------------------+--------------------+-----------+\n",
      "\n",
      "+-----------+------------+-----------+\n",
      "|string_null|string2_null|number_null|\n",
      "+-----------+------------+-----------+\n",
      "|      Hello|        null|        5.0|\n",
      "|       null|       World|        5.0|\n",
      "+-----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.na.fill(\"All null valus become this string\").show()\n",
    "myDf.na.fill(5.0).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------+\n",
      "|string_null|string2_null|number_null|\n",
      "+-----------+------------+-----------+\n",
      "|      Hello|        null|        5.0|\n",
      "|   No Value|       World|        5.0|\n",
      "+-----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 딕셔너리 타입을 사용해서 다수의 컬럼에 fill 메서드를 적용 \"\"\"\n",
    "fill_cols_vals = {\"number_null\": 5.0, \"string_null\": \"No Value\"}\n",
    "myDf.na.fill(fill_cols_vals).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7.5 replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------+\n",
      "|string_null|string2_null|number_null|\n",
      "+-----------+------------+-----------+\n",
      "|     Hello!|        null|        5.0|\n",
      "|       null|       World|       null|\n",
      "+-----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 조건에 따라 다른 값으로 대체 \"\"\"\n",
    "myDf.na.replace([\"Hello\"], [\"Hello!\"], \"string_null\").show() # null을 지정하는 방법은?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8 정렬하기\n",
    "5장(p.145의) asc_nulls_first, desc_nulls_first, asc_nulls_last, desc_nulls_last 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.9 복합 데이터 다루기\n",
    "+ 복합 데이터 타입에는 구조체(struct), 배열(array), 맵(map)이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.9.1 구조체\n",
    "+ DataFrame 내부의 DataFrame\n",
    "+ 다수의 컬럼을 괄호로 묶어 생성 가능\n",
    "+ 복합 데이터 타입은 다른 DataFrame을 조회하는 것과 동일하게 사용할 수 있음. 유일한 차이점은 문법에 점(.)을 사용하거나 getField 메서드를 사용한다는 것\n",
    "+ (*) 문자로 모든 값을 조회할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|complex                                      |\n",
      "+---------------------------------------------+\n",
      "|[WHITE HANGING HEART T-LIGHT HOLDER, 536365] |\n",
      "|[WHITE METAL LANTERN, 536365]                |\n",
      "|[CREAM CUPID HEARTS COAT HANGER, 536365]     |\n",
      "|[KNITTED UNION FLAG HOT WATER BOTTLE, 536365]|\n",
      "|[RED WOOLLY HOTTIE WHITE HEART., 536365]     |\n",
      "+---------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "\n",
    "complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.createOrReplaceTempView(\"complexDF\")\n",
    "complexDF.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|         Description|InvoiceNo|\n",
      "+--------------------+---------+\n",
      "|WHITE HANGING HEA...|   536365|\n",
      "| WHITE METAL LANTERN|   536365|\n",
      "|CREAM CUPID HEART...|   536365|\n",
      "|KNITTED UNION FLA...|   536365|\n",
      "|RED WOOLLY HOTTIE...|   536365|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complexDF.select(\"complex.Description\", \"complex.InvoiceNo\") # 모두 동일\n",
    "complexDF.select(col(\"complex\").getField(\"Description\"), col(\"complex\").getField(\"InvoiceNo\"))\n",
    "complexDF.select(\"complex.*\")\n",
    "complexDF.select(col(\"complex.*\"))\n",
    "complexDF.selectExpr(\"complex.*\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.9.2 배열\n",
    "+ 데이터에서 Description 컬럼의 모든 단어를 하나의 로우로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ split 함수에 구분자(delimiter)를 인수로 전달해 배열로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|split(Description,  , -1)|\n",
      "+-------------------------+\n",
      "|     [WHITE, HANGING, ...|\n",
      "|     [WHITE, METAL, LA...|\n",
      "+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 컬럼을 배열로 변환 \"\"\"\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "df.select(split(col(\"Description\"), \" \")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|array_col[0]|\n",
      "+------------+\n",
      "|       WHITE|\n",
      "|       WHITE|\n",
      "+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 배열값의 조회 \"\"\"\n",
    "df.select(split(col(\"Description\"), \" \").alias(\"array_col\"))\\\n",
    "    .selectExpr(\"array_col[0]\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 배열의 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|size(split(Description,  , -1))|\n",
      "+-------------------------------+\n",
      "|                              5|\n",
      "|                              3|\n",
      "+-------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" size 함수 \"\"\"\n",
    "from pyspark.sql.functions import size\n",
    "\n",
    "df.select(size(split(col(\"Description\"), \" \"))).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### array_contains\n",
    "+ array_contains 함수를 사용해 배열에 특정 값이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+\n",
      "|array_contains(split(Description,  , -1), WHITE)|\n",
      "+------------------------------------------------+\n",
      "|                                            true|\n",
      "|                                            true|\n",
      "+------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### explode\n",
    "+ 배열 타입의 컬럼을 입력받고 컬럼의 배열값에 포함된 모든 값을 로우로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3108\n",
      "14414\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "explodedDf = df.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\\\n",
    "                .withColumn(\"exploded\", explode(col(\"splitted\")))\\\n",
    "                .select(\"Description\", \"InvoiceNo\", \"exploded\") # 모든 단어가 하나의 로우로 전환됨\n",
    "\n",
    "print(df.select(\"Description\").count())\n",
    "print(explodedDf.select(\"exploded\").count()) # 로우 수가 다름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------+\n",
      "|         Description|InvoiceNo|exploded|\n",
      "+--------------------+---------+--------+\n",
      "|WHITE HANGING HEA...|   536365|   WHITE|\n",
      "|WHITE HANGING HEA...|   536365| HANGING|\n",
      "|WHITE HANGING HEA...|   536365|   HEART|\n",
      "|WHITE HANGING HEA...|   536365| T-LIGHT|\n",
      "|WHITE HANGING HEA...|   536365|  HOLDER|\n",
      "| WHITE METAL LANTERN|   536365|   WHITE|\n",
      "| WHITE METAL LANTERN|   536365|   METAL|\n",
      "| WHITE METAL LANTERN|   536365| LANTERN|\n",
      "|CREAM CUPID HEART...|   536365|   CREAM|\n",
      "|CREAM CUPID HEART...|   536365|   CUPID|\n",
      "+--------------------+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explodedDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14414"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explodedDf.select(\"Description\", \"exploded\").count() # 큰 쪽으로 카운드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Description='WHITE HANGING HEART T-LIGHT HOLDER', exploded='WHITE'),\n",
       " Row(Description='WHITE HANGING HEART T-LIGHT HOLDER', exploded='HANGING'),\n",
       " Row(Description='WHITE HANGING HEART T-LIGHT HOLDER', exploded='HEART'),\n",
       " Row(Description='WHITE HANGING HEART T-LIGHT HOLDER', exploded='T-LIGHT'),\n",
       " Row(Description='WHITE HANGING HEART T-LIGHT HOLDER', exploded='HOLDER'),\n",
       " Row(Description='WHITE METAL LANTERN', exploded='WHITE'),\n",
       " Row(Description='WHITE METAL LANTERN', exploded='METAL'),\n",
       " Row(Description='WHITE METAL LANTERN', exploded='LANTERN'),\n",
       " Row(Description='CREAM CUPID HEARTS COAT HANGER', exploded='CREAM'),\n",
       " Row(Description='CREAM CUPID HEARTS COAT HANGER', exploded='CUPID')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explodedDf.select(\"Description\", \"exploded\").take(10) # Description 컬럼이 Group이 되어 중복됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.9.3 맵\n",
    "+ map 함수와 컬럼의 키-값 쌍을 이용해 생성\n",
    "+ 적합한 키를 사용해 데이터를 조회할 수 있으며, 해당키가 없다면 null값을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(complex_map={'WHITE HANGING HEART T-LIGHT HOLDER': '536365'}),\n",
       " Row(complex_map={'WHITE METAL LANTERN': '536365'})]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 맵 생성 \"\"\"\n",
    "from pyspark.sql.functions import create_map\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|complex_map[WHITE METAL LANTERN]|\n",
      "+--------------------------------+\n",
      "|                            null|\n",
      "|                          536365|\n",
      "|                            null|\n",
      "|                            null|\n",
      "|                            null|\n",
      "+--------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 맵의 데이터 조회 \"\"\"\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    "    .selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                 key| value|\n",
      "+--------------------+------+\n",
      "|WHITE HANGING HEA...|536365|\n",
      "| WHITE METAL LANTERN|536365|\n",
      "+--------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 맵의 분해 \"\"\"\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    "    .selectExpr(\"explode(complex_map)\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.10 JSON 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Json 컬럼 생성 \"\"\"\n",
    "jsonDF = spark.range(1).selectExpr(\"\"\"\n",
    "    '{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ get_json_object 함수로 JSON 객체(딕셔너리나 배열)를 인라인 쿼리로 조회할 수 있음\n",
    "+ 중첩이 없는 단일 수준의 JSON 객체라면 json_tuple을 사용할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|jsonString                                 |\n",
      "+-------------------------------------------+\n",
      "|{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}|\n",
      "+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------+\n",
      "|column|c0                     |\n",
      "+------+-----------------------+\n",
      "|2     |{\"myJSONValue\":[1,2,3]}|\n",
      "+------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 인라인 쿼리로 JSON 조회하기 \"\"\"\n",
    "from pyspark.sql.functions import get_json_object, json_tuple\n",
    "\n",
    "jsonDF.select(\n",
    "    get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\").alias(\"column\"), # $ 의 의미가 뭐지??\n",
    "    json_tuple(col(\"jsonString\"), \"myJSONKey\")\n",
    ").show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(to_json(myStruct)='{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE HANGING HEART T-LIGHT HOLDER\"}'),\n",
       " Row(to_json(myStruct)='{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE METAL LANTERN\"}'),\n",
       " Row(to_json(myStruct)='{\"InvoiceNo\":\"536365\",\"Description\":\"CREAM CUPID HEARTS COAT HANGER\"}')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" StructType을 Json 문자열로 변경 \"\"\"\n",
    "from pyspark.sql.functions import to_json\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
    "    .select(to_json(col(\"myStruct\"))).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|  from_json(newJSON)|             newJSON|\n",
      "+--------------------+--------------------+\n",
      "|[536365, WHITE HA...|{\"InvoiceNo\":\"536...|\n",
      "|[536365, WHITE ME...|{\"InvoiceNo\":\"536...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Json 문자열을 객체로 변환 \"\"\"\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "parseSchema = StructType([\n",
    "    StructField(\"InvoiceNo\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True)\n",
    "])\n",
    "\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
    "    .select(to_json(col(\"myStruct\")).alias(\"newJSON\"))\\\n",
    "    .select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\")).show(2) # 키를 컬럼명으로 값을 로우로 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.11 사용자 정의 함수 \n",
    "+ User defined function(UDF)는 레포트별로 데이터를 처리하는 함수이며, SparkSession이나 Context에서 사용할 수 있도록 임시 함수 형태로 등록됨\n",
    "+ 내장 함수가 제공하는 코드 생성 기능의 장점을 활용할 수 없어 약간의 성능 저하 발생\n",
    "+ 언어별로 성능차이가 존재, 파이썬에서도 사용할 수 있으므로 자바나 스칼라도 함수 작성을 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" UDF 사용하기 \"\"\"\n",
    "udfExDF = spark.range(5).toDF(\"num\")\n",
    "def power3(double_value):\n",
    "    return double_value ** 3\n",
    "power3(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" UDF 등록 및 사용 \"\"\"\n",
    "from pyspark.sql.functions import udf\n",
    "power3udf = udf(power3)\n",
    "\n",
    "udfExDF.select(power3udf(col(\"num\"))).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스칼라에서 등록되 사용자 정의 함수를 파이썬에서 활용:\n",
    "\n",
    "https://www.cyanny.com/2017/09/15/spark-use-scala-udf-udaf-in-pyspark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|power3py(num)|\n",
      "+-------------+\n",
      "|         null|\n",
      "|         null|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 사용자 정의 함수를 스파크 SQL로 등록하면 문자열 표현식에서 사용할 수 있게 됨\n",
    "spark.udf.register(\"power3py\", power3, DoubleType()) # 반환 타입을 Double로 변경하면 null 이 반영됨\n",
    "udfExDF.selectExpr(\"power3py(num)\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power3_f(double_value):\n",
    "    return float(double_value ** 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|power3pyf(num)|\n",
      "+--------------+\n",
      "|           0.0|\n",
      "|           1.0|\n",
      "+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 사용자 정의 함수를 스파크 SQL로 등록하면 문자열 표현식에서 사용할 수 있게 됨\n",
    "spark.udf.register(\"power3pyf\", power3_f, DoubleType()) \n",
    "udfExDF.selectExpr(\"power3pyf(num)\").show(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
